from pyspark.sql import SparkSession
import re
from pyspark.sql import functions as F

# Create a widget to pass the business_date
dbutils.widgets.text("business_date", "")
business_date_filter = dbutils.widgets.get("business_date")

# Function to extract business_date, upload_date, and time from the filename
def parse_filename(file_name):
    pattern = r"mcr_ytd_cle_(\d{8})_(\d{8})_(\d{6})"
    match = re.match(pattern, file_name)
    if match:
        business_date = match.group(1)
        upload_date = match.group(2)
        file_time = match.group(3)
        return business_date, upload_date, file_time
    return None, None, None

# List the files from the Azure Storage path
file_list = dbutils.fs.ls("your_azure_storage_path")

# Filter files matching the passed business_date from widget
file_info = []
for file in file_list:
    file_name = file.name
    business_date, upload_date, file_time = parse_filename(file_name)
    if business_date == business_date_filter:
        file_info.append((file_name, business_date, upload_date, file_time))

# Sort files based on upload_date and file_time to get the latest ones
sorted_files = sorted(file_info, key=lambda x: (x[2], x[3]), reverse=True)

# Process each sorted file and write to a partitioned table
for file_name, business_date, upload_date, file_time in sorted_files:
    file_path = f"your_azure_storage_path/{file_name}"
    
    # Load the file into a DataFrame
    df = spark.read.text(file_path)
    
    # Add the business_date column to the DataFrame (to be used for partitioning)
    df = df.withColumn("business_date", F.lit(business_date))
    
    # Write the DataFrame to the table, partitioned by business_date, in overwrite mode
    df.write.mode("overwrite").partitionBy("business_date").format("parquet").saveAsTable("your_table_name")
    
    # Optionally, you can control the overwrite behavior to keep only the latest files
    # by further filtering or ensuring latest records per business_date are retained.

# After writing, your table will be partitioned by business_date, and only the latest files (based on date and time) will be stored.