df = spark.read.json(spark.sparkContext.parallelize(json_data))

# Function to recursively flatten DataFrame
def flatten_df(df):
    complex_fields = {field.name: field.dataType for field in df.schema.fields if isinstance(field.dataType, (StructType, ArrayType))}
    
    while complex_fields:
        col_name, data_type = complex_fields.popitem()
        
        if isinstance(data_type, ArrayType) and isinstance(data_type.elementType, StructType):
            # Explode the array column
            df = df.withColumn(col_name, explode(col_name))
            # Add nested struct fields to the list of fields to process
            for field in data_type.elementType.fields:
                df = df.withColumn(f"{col_name}_{field.name}", col(f"{col_name}.{field.name}"))
            # Drop the original array column
            df = df.drop(col_name)
        elif isinstance(data_type, StructType):
            # Add nested struct fields to the list of fields to process
            for field in data_type.fields:
                df = df.withColumn(f"{col_name}_{field.name}", col(f"{col_name}.{field.name}"))
            # Drop the original struct column
            df = df.drop(col_name)
    
        # Update complex fields
        complex_fields.update({field.name: field.dataType for field in df.schema.fields if isinstance(field.dataType, (StructType, ArrayType))})
    
    return df

# Flatten the DataFrame
df_flattened = flatten_df(df)

# Show the resulting DataFrame
df_flattened.show(truncate=False)